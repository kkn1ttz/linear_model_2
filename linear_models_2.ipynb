{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12541d8f",
   "metadata": {},
   "source": [
    "Alohan'ny mamerina dia avereno atao Run ny notebook iray manontolo. Ny fanaovana azy dia red√©marrena mihitsy ny kernel aloha (jereo menubar, safidio **Kernel$\\rightarrow$Restart Kernel and Run All Cells**).\n",
    "\n",
    "Izay misy hoe `YOUR CODE HERE` na \"YOUR ANSWER HERE\" ihany no fenoina. Afaka manampy cells vaovao raha ilaina. Aza adino ny mameno references eo ambany raha ilaina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7998e75",
   "metadata": {},
   "source": [
    "## References\n",
    "Eto ilay references rehetra no apetraka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf9d839",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8a6cec-de9c-4bef-aa39-c3d9aae99b44",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63e719b8be6612035df8edf4b3c115da",
     "grade": false,
     "grade_id": "cell-aa89a6ac306b2622",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import cvxpy as cp\n",
    "from sklearn.linear_model import Lasso\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from random import randrange\n",
    "from sklearn.datasets import load_boston, load_diabetes, load_iris, load_digits, load_breast_cancer, make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def grad_check_sparse(f, x, analytic_grad, num_checks=12, h=1e-5, error=1e-9):\n",
    "    \"\"\"\n",
    "    sample a few random elements and only return numerical\n",
    "    in this dimensions\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(num_checks):\n",
    "        ix = tuple([randrange(m) for m in x.shape])\n",
    "\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h  # increment by h\n",
    "        fxph = f(x)  # evaluate f(x + h)\n",
    "        x[ix] = oldval - h  # increment by h\n",
    "        fxmh = f(x)  # evaluate f(x - h)\n",
    "        x[ix] = oldval  # reset\n",
    "\n",
    "        grad_numerical = (fxph - fxmh) / (2 * h)\n",
    "        grad_analytic = analytic_grad[ix]\n",
    "        rel_error = abs(grad_numerical - grad_analytic) / (\n",
    "            abs(grad_numerical) + abs(grad_analytic)\n",
    "        )\n",
    "        print(\n",
    "            \"numerical: %f analytic: %f, relative error: %e\"\n",
    "            % (grad_numerical, grad_analytic, rel_error)\n",
    "        )\n",
    "        assert rel_error < error\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adf63c2-fba8-4b87-b410-33a91adeb66c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dbacabe5fa3bf5221642a8dd304f2c92",
     "grade": false,
     "grade_id": "cell-a406f93bae267ca8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Alternative Linear regression\n",
    "\n",
    "Implement linear regression using the following alternative loss instead of MSE:\n",
    "\n",
    "$$Loss(\\mathbf{w})= \\frac{1}{N} \\sum_{i=1}^N h_\\epsilon(\\mathbf{w}^\\top\\mathbf{x}_i +b - \\mathbf{y}_i) + \\lambda ||\\mathbf{w}||^2_2$$\n",
    "\n",
    "where $$h_\\epsilon(r) =  \\begin{cases}\n",
    "    r^2/2 & \\text{if } |r|\\le \\epsilon \\\\ \n",
    "    \\epsilon|r|-\\epsilon^2/2 & \\text{if } |r| \\gt \\epsilon\n",
    "\\end{cases}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16df46b-e2bf-4d16-8d56-470e1726441b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "785a7b17eaead3da12963ed20e9f1345",
     "grade": false,
     "grade_id": "cell-7ada948d8b1ea6fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = load_boston()\n",
    "X_train1, y_train1 = data.data, data.target\n",
    "w1 = np.random.randn(X_train1.shape[1]) * 0.0001\n",
    "b1 = np.random.randn(1) * 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3d3e57-26fd-4f34-be5b-73026e5729e9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9787626c2bb3858e696e34ef33a5600c",
     "grade": false,
     "grade_id": "cell-58a0bd79b20f3870",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def alternative_loss_lr_naive(w, b, X, y, epsilon=1.35, alpha=0.0001):\n",
    "    \"\"\"\n",
    "    Naive loss for all observations\n",
    "    \n",
    "    Inputs:\n",
    "    - w: array of shape (D,) containing weights\n",
    "    - b: float bias \n",
    "    - X: array of shape (N, D) containing a minibatch of data\n",
    "    - y: array of shape (N,) containing training labels \n",
    "    - epsilon: float\n",
    "    - alpha: regularization\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    loss = 0.0\n",
    "    dw = np.zeros_like(w)\n",
    "    db = 0.0\n",
    "    \n",
    "    for i in range(N):\n",
    "        # Calculate the prediction\n",
    "        prediction = np.dot(X[i], w) + b\n",
    "        # Calculate the residual\n",
    "        residual = prediction - y[i]\n",
    "        # Apply the piecewise function for the loss\n",
    "        if abs(residual) <= epsilon:\n",
    "            loss += (residual ** 2) / 2\n",
    "            # Gradient update for weights and bias inside the epsilon bound\n",
    "            dw += residual * X[i]\n",
    "            db += residual\n",
    "        else:\n",
    "            loss += epsilon * abs(residual) - (epsilon ** 2) / 2\n",
    "            # Gradient update for weights and bias outside the epsilon bound\n",
    "            dw += epsilon * np.sign(residual) * X[i]\n",
    "            db += epsilon * np.sign(residual)\n",
    "            \n",
    "    # Add regularization to the loss\n",
    "    loss /= N\n",
    "    loss += alpha * np.sum(w ** 2)\n",
    "    \n",
    "    # Add regularization to the gradients\n",
    "    dw /= N\n",
    "    dw += 2 * alpha * w\n",
    "    db /= N\n",
    "    \n",
    "    return loss, dw, np.array(db).reshape(1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915b71cb-7de8-448a-914d-1c5bae7cd79a",
   "metadata": {},
   "source": [
    "## without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b89839-ef35-4cea-8939-c7cd6badf33f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cdd657b93831682117f34bed7abf82d8",
     "grade": true,
     "grade_id": "cell-e411b69ad1c6b508",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss, dw1, db1 = alternative_loss_lr_naive(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=0)\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: alternative_loss_lr_naive(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, dw1, 15, error=1e-8)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: alternative_loss_lr_naive(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, db1, 15, error=1e-8)\n",
    "\n",
    "\n",
    "# Large epsilon\n",
    "large_eps_loss, large_eps_dw1, large_eps_db1 = alternative_loss_lr_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=0)\n",
    "\n",
    "print(\"Gradient check w large epsilon\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: alternative_loss_lr_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, large_eps_dw1, 15, error=1e-8)\n",
    "\n",
    "print(\"Gradient check bias large epsilon\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: alternative_loss_lr_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, large_eps_db1, 15, error=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96d11db-c9ed-4eb7-8ce2-9bbf50c0210d",
   "metadata": {},
   "source": [
    " ## with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e6ff18-601c-4e9e-89a0-fff400c15d68",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a93478ed5277202b6806323f2089899c",
     "grade": true,
     "grade_id": "cell-9c24cece28433a31",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss, dw1, db1 = alternative_loss_lr_naive(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=1)\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: alternative_loss_lr_naive(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, dw1, 15, error=1e-8)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: alternative_loss_lr_naive(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, db1, 15, error=1e-8)\n",
    "\n",
    "\n",
    "# Large epsilon\n",
    "large_eps_loss, large_eps_dw1, large_eps_db1 = alternative_loss_lr_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=1)\n",
    "\n",
    "print(\"Gradient check w large epsilon\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: alternative_loss_lr_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, large_eps_dw1, 15, error=1e-8)\n",
    "\n",
    "print(\"Gradient check bias large epsilon\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: alternative_loss_lr_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, large_eps_db1, 15, error=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f7a31b-0c34-4992-95c4-362626b9f340",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ea46d80e1350cf3e44c3fa85e6c01f5",
     "grade": false,
     "grade_id": "cell-445ae900273c82f4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def alternative_loss_lr_vectorized(w, b, X, y, epsilon=1.35, alpha=0.0001):\n",
    "    \"\"\"\n",
    "    Vectorized for all observations\n",
    "    \n",
    "    Inputs:\n",
    "    - w: array of shape (D,) containing weights\n",
    "    - b: float bias \n",
    "    - X: array of shape (N, D) containing a minibatch of data\n",
    "    - y: array of shape (N,) containing training labels \n",
    "    - epsilon: float\n",
    "    - alpha: regularization\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "\n",
    "    # Compute predictions\n",
    "    predictions = X.dot(w) + b\n",
    "    \n",
    "    # Compute residuals\n",
    "    residuals = predictions - y\n",
    "    \n",
    "    # Apply the piecewise function h_epsilon(r) for loss\n",
    "    abs_residuals = np.abs(residuals)\n",
    "    quadratic_part = np.where(abs_residuals <= epsilon, 0.5 * residuals ** 2, 0)\n",
    "    linear_part = np.where(abs_residuals > epsilon, epsilon * (abs_residuals - 0.5 * epsilon), 0)\n",
    "    loss = np.sum(quadratic_part + linear_part) / N\n",
    "    \n",
    "    # Add regularization to the loss\n",
    "    loss += alpha * np.sum(w ** 2)\n",
    "    \n",
    "    # Compute gradient\n",
    "    indicator_quadratic = abs_residuals <= epsilon\n",
    "    indicator_linear = abs_residuals > epsilon\n",
    "    \n",
    "    dquadratic = residuals * indicator_quadratic\n",
    "    dlinear = epsilon * np.sign(residuals) * indicator_linear\n",
    "    \n",
    "    dw = (X.T.dot(dquadratic + dlinear)) / N + 2 * alpha * w\n",
    "    db = np.sum(dquadratic + dlinear) / N\n",
    "    \n",
    "    return loss, dw, db.reshape(1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0281bb0e-0215-4ec9-9bb0-54f2d2888fdb",
   "metadata": {},
   "source": [
    "## without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a4cf8e-64d3-4dff-ad7b-7418c5c77401",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76e919bc752ce9a776b264cec9938e31",
     "grade": true,
     "grade_id": "cell-4568706b3fcae77c",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss, dw1, db1 = alternative_loss_lr_vectorized(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=0)\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: alternative_loss_lr_vectorized(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, dw1, 15, error=1e-8)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: alternative_loss_lr_vectorized(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, db1, 15, error=1e-8)\n",
    "\n",
    "\n",
    "# Large epsilon\n",
    "large_eps_loss, large_eps_dw1, large_eps_db1 = alternative_loss_lr_vectorized(w1, b1, X_train1, y_train1, epsilon=135, alpha=0)\n",
    "\n",
    "print(\"Gradient check w large epsilon\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: alternative_loss_lr_vectorized(w1, b1, X_train1, y_train1, epsilon=135, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, large_eps_dw1, 15, error=1e-8)\n",
    "\n",
    "print(\"Gradient check bias large epsilon\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: alternative_loss_lr_vectorized(w1, b1, X_train1, y_train1, epsilon=135, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, large_eps_db1, 15, error=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c02ecb-758a-445c-9739-e370f0cc6de8",
   "metadata": {},
   "source": [
    "## with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a0dbae-ed92-4e53-a1dd-be35673b6be6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29c030a1e0b485b0d0f9afab2f3bfd19",
     "grade": true,
     "grade_id": "cell-c66289004f6aa9d9",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss, dw1, db1 = alternative_loss_lr_vectorized(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=1)\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: alternative_loss_lr_vectorized(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, dw1, 15, error=1e-8)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: alternative_loss_lr_vectorized(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, db1, 15, error=1e-8)\n",
    "\n",
    "\n",
    "# Large epsilon\n",
    "large_eps_loss, large_eps_dw1, large_eps_db1 = alternative_loss_lr_vectorized(w1, b1, X_train1, y_train1, epsilon=135, alpha=1)\n",
    "\n",
    "print(\"Gradient check w large epsilon\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: alternative_loss_lr_vectorized(w1, b1, X_train1, y_train1, epsilon=135, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, large_eps_dw1, 15, error=1e-8)\n",
    "\n",
    "print(\"Gradient check bias large epsilon\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: alternative_loss_lr_vectorized(w1, b1, X_train1, y_train1, epsilon=135, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, large_eps_db1, 15, error=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd2fa12-5e42-4971-8194-380f70e3254f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "70a98bc9e585dfab6017e489c82991d1",
     "grade": false,
     "grade_id": "cell-de41e362b7f0b285",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LinearModelRegression():\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def train(self, X, y, learning_rate=1e-3, alpha=0.0001, num_iters=100, batch_size=200, verbose=False):\n",
    "        N, d = X.shape\n",
    "        \n",
    "        if self.w is None: # Initialization\n",
    "            self.w = 0.001 * np.random.randn(d)\n",
    "            self.b = 0.0\n",
    "\n",
    "        # Run stochastic gradient descent to optimize w\n",
    "        \n",
    "        loss_history = []\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "                                                               \n",
    "            # Sample batch_size elements in X_batch and y_batch\n",
    "            indices = np.random.choice(N, batch_size)\n",
    "            X_batch = X[indices]\n",
    "            y_batch = y[indices]\n",
    "            \n",
    "            # evaluate loss and gradient\n",
    "            loss, dw, db = self.loss(X_batch, y_batch, alpha)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            # perform parameter update\n",
    "            # Update the weights w and bias b using the gradient and the learning rate. \n",
    "            self.w -= learning_rate * dw\n",
    "            self.b -= learning_rate * db\n",
    "            \n",
    "            if verbose and it % 10000 == 0:\n",
    "                print(\"iteration %d / %d: loss %f\" % (it, num_iters, loss))\n",
    "                \n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        pass\n",
    "\n",
    "class AlternativeLinearRegression(LinearModelRegression):\n",
    "    \"\"\" Linear regression \"\"\"\n",
    "\n",
    "    def loss(self, X_batch, y_batch, alpha):\n",
    "        return alternative_loss_lr_vectorized(self.w, self.b, X_batch, y_batch, alpha=alpha)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Use the learned weights and bias to make predictions.\n",
    "        return X.dot(self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20801f78-b598-44fa-b4aa-383c5abde0ef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7011c3efb928dc22468a6f284e8bbae3",
     "grade": true,
     "grade_id": "cell-79673dd4eb191c3a",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train1 = scaler.fit_transform(X_train1)\n",
    "\n",
    "model = AlternativeLinearRegression()\n",
    "model.train(X_train1, y_train1, num_iters=75000, batch_size=64, learning_rate=1e-2, verbose=True)\n",
    "pred = model.predict(X_train1)\n",
    "mse = mean_squared_error(pred, y_train1)\n",
    "\n",
    "print(\"MSE gradient descent model :\", mse)\n",
    "assert mse < 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48e0159-a76c-468a-a9b4-53d1540ded59",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Alternative multiclass classification\n",
    "\n",
    "Implement classification using the following loss:\n",
    "\n",
    "$$L(\\mathbf{W}) = \\sum_{i=1}^N \\sum_{j \\neq y_i} \\max(0, s_j - s_{y_i} + 1) + \\lambda||\\mathbf{w}||^2_2$$\n",
    "$$\\text{where } s_j = (f(\\mathbf{x}_i;\\mathbf{W}))_j = (\\mathbf{W}\\mathbf{x}_i)_j \\text{ is the score for the j-th class}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b532b6ff-325b-4289-8429-f1fa417eecd5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "506616c8b4cad00d19f3808efb818666",
     "grade": false,
     "grade_id": "cell-ecfd857cb17d39c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "W = np.random.randn(X.shape[1], 3) * 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c78eb3-f9c3-4a56-bdd3-037b7328ba1e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b08efc872298fc396c4525b1d2235c1",
     "grade": false,
     "grade_id": "cell-de5fbc82305bd304",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def alternative_classification_loss_naive(W, X, y, alpha):\n",
    "    \"\"\"\n",
    "    Multiclass Naive loss function WITH FOR LOOPS\n",
    "\n",
    "    Inputs:\n",
    "    - W: array of shape (D, C) containing weights\n",
    "    - X: array of shape (N, D) containing a minibatch of data\n",
    "    - y: array of shape (N,) containing training labels\n",
    "    - alpha: (float) regularization \n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss as single float\n",
    "    - gradient with respect to weights W;  same shape as W\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialization\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "    N, D = X.shape\n",
    "    C = W.shape[1]\n",
    "    \n",
    "    for i in range(N): # Iterate over all samples\n",
    "        scores = X[i].dot(W)\n",
    "        correct_class_score = scores[y[i]]\n",
    "        \n",
    "        for j in range(C): # Iterate over all classes\n",
    "            if j == y[i]:\n",
    "                continue # Skip the correct class to only calculate loss on incorrect classes\n",
    "            margin = scores[j] - correct_class_score + 1 # Note the +1 for margin\n",
    "            if margin > 0:\n",
    "                loss += margin\n",
    "                dW[:, j] += X[i] # Gradient update for incorrect class\n",
    "                dW[:, y[i]] -= X[i] # Gradient update for correct class\n",
    "    \n",
    "    # Average loss over the batch and add regularization\n",
    "    loss /= N\n",
    "    loss += alpha * np.sum(W * W)\n",
    "    \n",
    "    # Average gradients over the batch and add regularization gradient\n",
    "    dW /= N\n",
    "    dW += 2 * alpha * W # L2 regularization gradient\n",
    "\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc7557e-8de3-4d9c-b55f-aafd3cd812e8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e6b2b2547ee161c874182061194ca47",
     "grade": true,
     "grade_id": "cell-e60c7cd2ced528dd",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NO REGLARIZATION\n",
    "loss, dW = alternative_classification_loss_naive(W, X, y, 0.0)\n",
    "\n",
    "f = lambda W: alternative_classification_loss_naive(W, X, y, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, dW, error=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af38e734-2f39-4b87-9324-df76f1134a6e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c654d26ba9d47359d9b380578775d58f",
     "grade": true,
     "grade_id": "cell-ee3ee976a055e1ab",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# With REGLARIZATION\n",
    "loss, dW = alternative_classification_loss_naive(W, X, y, 2)\n",
    "\n",
    "f = lambda W: alternative_classification_loss_naive(W, X, y, 2)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, dW, error=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2a7930-8afd-4ab2-ae5a-ef6400d9136a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a5b92c37484d9316dc45fc614560c50",
     "grade": false,
     "grade_id": "cell-0cfb17c09247818c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def alternative_classification_loss_vectorized(W, X, y, alpha):\n",
    "    \"\"\"\n",
    "    Multiclass vectorized loss function WITHOUT FOR LOOPS\n",
    "\n",
    "    Inputs:\n",
    "    - W: array of shape (D, C) containing weights\n",
    "    - X: array of shape (N, D) containing a minibatch of data\n",
    "    - y: array of shape (N,) containing training labels\n",
    "    - alpha: (float) regularization \n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss as single float\n",
    "    - gradient with respect to weights W;  same shape as W\n",
    "    \"\"\"\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "\n",
    "    N = X.shape[0]\n",
    "    # Compute the scores\n",
    "    scores = X.dot(W)\n",
    "    \n",
    "    # Compute the correct class scores\n",
    "    correct_class_scores = scores[np.arange(N), y].reshape(-1, 1)\n",
    "    \n",
    "    # Compute margins\n",
    "    margins = np.maximum(0, scores - correct_class_scores + 1)\n",
    "    \n",
    "    # Do not consider correct class in loss\n",
    "    margins[np.arange(N), y] = 0\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = np.sum(margins) / N\n",
    "    # Add regularization to the loss\n",
    "    loss += alpha * np.sum(W * W)\n",
    "    \n",
    "    # Compute gradient\n",
    "    positive_margins = (margins > 0).astype(float)\n",
    "    # Count the number of classes that contribute to the loss for each sample\n",
    "    positive_margins[np.arange(N), y] -= np.sum(positive_margins, axis=1)\n",
    "    \n",
    "    # Compute the gradient\n",
    "    dW = X.T.dot(positive_margins) / N\n",
    "    # Add regularization to the gradient\n",
    "    dW += 2 * alpha * W\n",
    "\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6532e35-6253-4905-98cf-f4c4fa780af8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92f5c1e8924c8d2979dd17dd3cd6d986",
     "grade": true,
     "grade_id": "cell-8607b10af5824f0d",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NO REGLARIZATION\n",
    "loss, dW = alternative_classification_loss_vectorized(W, X, y, 0.0)\n",
    "\n",
    "f = lambda W: alternative_classification_loss_vectorized(W, X, y, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, dW, error=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbc1af6-1e26-488a-80bf-4b95cfb4fec9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac906a1fe3e4a505dc656f30e9426636",
     "grade": true,
     "grade_id": "cell-392863c245424209",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# REGLARIZATION\n",
    "loss, dW = alternative_classification_loss_vectorized(W, X, y, 2)\n",
    "\n",
    "f = lambda W: alternative_classification_loss_vectorized(W, X, y, 2)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, dW, error=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edad10e2-d795-4f42-a433-4dac4bd121f3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d7bda7514ebd428721fbf822b362437",
     "grade": false,
     "grade_id": "cell-090d40b36cbe2698",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LinearModelClassification():\n",
    "    def __init__(self, fit_intercept=True):\n",
    "        self.W = None\n",
    "        self.fit_intercept = fit_intercept\n",
    "\n",
    "    def train(self, X, y, learning_rate=1e-3, alpha=0, num_iters=100, batch_size=200, verbose=False):\n",
    "        if self.fit_intercept:\n",
    "            X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "            \n",
    "        N, d = X.shape\n",
    "        \n",
    "        C = (np.max(y) + 1) \n",
    "        if self.W is None: # Initialization\n",
    "            self.W = 0.001 * np.random.randn(d, C)\n",
    "\n",
    "        # Run stochastic gradient descent to optimize W\n",
    "        \n",
    "        loss_history = []\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "                                                               \n",
    "            # Sample batch_size elements in X_batch and y_batch\n",
    "            indices = np.random.choice(N, batch_size)\n",
    "            X_batch = X[indices]\n",
    "            y_batch = y[indices]\n",
    "            \n",
    "            # evaluate loss and gradient\n",
    "            loss, dW = self.loss(X_batch, y_batch, alpha)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            # perform parameter update                                                                \n",
    "            # Update the weights w using the gradient and the learning rate.          \n",
    "            self.W -= learning_rate * dW\n",
    "            \n",
    "            if verbose and it % 10000 == 0:\n",
    "                print(\"iteration %d / %d: loss %f\" % (it, num_iters, loss))\n",
    "                \n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        pass\n",
    "\n",
    "class AlternativeClassificationModel(LinearModelClassification):\n",
    "    \"\"\" Multiclass classification model \"\"\"\n",
    "\n",
    "    def loss(self, X_batch, y_batch, alpha):\n",
    "        return alternative_classification_loss_vectorized(self.W, X_batch, y_batch, alpha)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" \n",
    "        Inputs:\n",
    "        - X: array of shape (N, D) \n",
    "\n",
    "        Returns:\n",
    "        - y_pred: 1-dimensional array of length N, each element is an integer giving the predicted class \n",
    "        \"\"\"\n",
    "        if self.fit_intercept:\n",
    "            X = np.hstack([np.ones((X.shape[0], 1)), X])  # Add intercept term\n",
    "        scores = X.dot(self.W)\n",
    "        y_pred = np.argmax(scores, axis=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0e41cd-1c1d-4f30-ad00-da5fac9e0135",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc5561f6ae369bd743d4ff54e0cc43a1",
     "grade": true,
     "grade_id": "cell-af0f811ba9507047",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AlternativeClassificationModel()\n",
    "model.train(X, y, num_iters=75000, batch_size=64, learning_rate=1e-3, verbose=True)\n",
    "pred = model.predict(X)\n",
    "model_accuracy = accuracy_score(y, pred)\n",
    "print(model_accuracy)\n",
    "assert model_accuracy > 0.97"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038b2f22-eb59-4a9a-92da-31af14933f22",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Logistic regression with CVXPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c481f9-4223-4c7f-a708-e68f2caa6f7d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "46bfa377283619710bb37001d755d218",
     "grade": false,
     "grade_id": "cell-5e638560eb7e494f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data2 = load_breast_cancer()\n",
    "X2, y2 = data2.data, data2.target\n",
    "scaler = StandardScaler()\n",
    "X2 = scaler.fit_transform(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efc598b-56de-4265-a961-148473f128ec",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f79e8d452c9cd2af78fbfeb84b51e284",
     "grade": false,
     "grade_id": "cell-4a7f06bee180cd93",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "class LogisticRegressionCVXPY():\n",
    "    def __init__(self, fit_intercept=True, alpha=1.0):\n",
    "        self.w = None\n",
    "        self.b = None if fit_intercept else 0\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        N, D = X.shape\n",
    "        w = cp.Variable(D)\n",
    "        b = cp.Variable() if self.fit_intercept else 0\n",
    "        \n",
    "        # Formulate the logistic loss function\n",
    "        log_likelihood = cp.sum(\n",
    "            cp.multiply(y, X @ w + b) - \n",
    "            cp.logistic(X @ w + b)\n",
    "        )\n",
    "        \n",
    "        # Add L2 regularization\n",
    "        L2_regularization = (self.alpha / 9) * cp.norm(w, 2)**2\n",
    "        \n",
    "        # Define the objective function to maximize log likelihood\n",
    "        objective = cp.Maximize(log_likelihood / N - L2_regularization)\n",
    "        prob = cp.Problem(objective)\n",
    "        \n",
    "        # Solve the optimization problem\n",
    "        prob.solve(solver=cp.ECOS, abstol=1e-10, reltol=1e-9, feastol=1e-10)\n",
    "        \n",
    "        # Save the learned parameters\n",
    "        self.w = w.value\n",
    "        if self.fit_intercept:\n",
    "            self.b = b.value\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\" Return prediction labels vector of 0 or 1 \"\"\"\n",
    "        if self.fit_intercept and self.b is not None:\n",
    "            preds = sigmoid(X @ self.w + self.b)\n",
    "        else:\n",
    "            preds = sigmoid(X @ self.w)\n",
    "        # Convert probabilities to 0 or 1 based on threshold of 0.5\n",
    "        return (preds >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcce374c-7e1a-430a-b0c0-c5895d63527c",
   "metadata": {},
   "source": [
    "### without bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2d37d7-4939-432c-a166-b6097e836837",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f6b0d572a6ed8496b213830ab0e71fd",
     "grade": true,
     "grade_id": "cell-201adaa70664311a",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LogisticRegressionCVXPY(alpha=1e-3, fit_intercept=False)\n",
    "model.fit(X2, y2)\n",
    "pred = model.predict(X2)\n",
    "accuracy = accuracy_score(y2, pred)\n",
    "print(accuracy)\n",
    "assert accuracy >= 0.98"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4863da8e-f7fa-413a-b28e-3915ce96aad9",
   "metadata": {},
   "source": [
    "### with bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b7c1a4-a56a-41d9-8467-f52912f1a0a6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a86bf2c0d008fb5486b00a0cb3b9382",
     "grade": true,
     "grade_id": "cell-81e8d6c923a9bf29",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LogisticRegressionCVXPY(alpha=1e-3, fit_intercept=True)\n",
    "model.fit(X2, y2)\n",
    "pred = model.predict(X2)\n",
    "accuracy = accuracy_score(y2, pred)\n",
    "accuracy = accuracy\n",
    "print(accuracy)\n",
    "assert accuracy >= 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78724a5-6388-44ff-9df7-42779c6ad39d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Alternative Binary classification CVXPY\n",
    " \n",
    "Implement a binary classification model with CVXPY whose parameters are obtained by: (label is 1 and -1 instead of 0 and 1)\n",
    "\n",
    "$$\\min_{\\mathbf{w},b}\\frac{1}{2}||\\mathbf{w}||^2$$\n",
    "$$\\text{s.t } y_i(\\mathbf{w}^{\\top}\\mathbf{x}_i + b) \\ge 1, \\ i=1...N$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3fc8c2-8697-49cf-b52d-513e498c4157",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5fa0d2af2bd6aecccdc4d078468390a1",
     "grade": false,
     "grade_id": "cell-af61a3b1d42b02bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X3, y3 = make_blobs(n_samples=300, centers=2, n_features=12, random_state=47)\n",
    "scaler = StandardScaler()\n",
    "X3 = scaler.fit_transform(X3)\n",
    "y3[y3 == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76642ba-80ee-4d54-a9f1-571f8cc7df74",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0523854942d88340c0e13b1a19b7e29",
     "grade": false,
     "grade_id": "cell-7ce17511e3c6bd60",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BinaryClassificationModel():\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.b = 0  # Initialized to 0, but will be optimized\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        N, D = X.shape  # Number of samples and features\n",
    "        w = cp.Variable(D)  # Weight vector\n",
    "        b = cp.Variable()  # Bias term\n",
    "\n",
    "        # Loss function: Sum of hinge losses for all samples\n",
    "        # Hinge loss for binary classification: max(0, 1 - y_i * (X_i @ w + b))\n",
    "        loss = cp.sum(cp.pos(1 - cp.multiply(y, X @ w + b)))\n",
    "        \n",
    "        # Define and solve the optimization problem\n",
    "        problem = cp.Problem(cp.Minimize(loss))\n",
    "        problem.solve()\n",
    "        \n",
    "        # Store the optimized parameters\n",
    "        self.w = w.value\n",
    "        self.b = b.value\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return the predicted label 1 or -1\"\"\"\n",
    "        y_pred = X @ self.w + self.b\n",
    "        return np.sign(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec690b1-8a64-4ec6-94b5-d4f86e0168a8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd712bde6ae73d7d03d48e5343f81269",
     "grade": true,
     "grade_id": "cell-6d09e6f5e0fb5452",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BinaryClassificationModel()\n",
    "model.fit(X3, y3)\n",
    "pred = model.predict(X3)\n",
    "accuracy = accuracy_score(y3, pred)\n",
    "print(accuracy)\n",
    "assert accuracy == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b825019b-0d08-46a1-9002-85aaba46f621",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Alternative  Binary classification 2 CVXPY\n",
    " \n",
    "Implement a binary classification model with CVXPY whose parameters minimize the loss\n",
    "\n",
    "$$L(\\mathbf{w},b) = \\frac{1}{N} \\sum_{i=1}^N \\max(0, y_i(\\mathbf{w}^{\\top}\\mathbf{x}_i + b)) + \\lambda||\\mathbf{w}||^2_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbae774-34b8-48a7-92d8-d5b80e1242df",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f96aa9cebef49bd38deccd7785bd197",
     "grade": false,
     "grade_id": "cell-172a617e4afb59a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data4 = load_breast_cancer()\n",
    "X4, y4 = data4.data, data4.target\n",
    "scaler = StandardScaler()\n",
    "X4 = scaler.fit_transform(X4)\n",
    "y4[y4 == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad2d585-fcfd-4720-b9aa-3dd5b41f8d9b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c91102fa041f937fd1ed7a9fe91a746b",
     "grade": false,
     "grade_id": "cell-83693192e6e4040d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BinaryClassificationModel2():\n",
    "    def __init__(self, alpha=0):\n",
    "        self.w = None\n",
    "        self.b = 0  # Initialized to 0, but will be optimized\n",
    "        self.alpha = alpha  # Regularization strength\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        N, D = X.shape  # Number of samples and features\n",
    "        w = cp.Variable(D)  # Weight vector\n",
    "        b = cp.Variable()   # Bias term\n",
    "\n",
    "        # Loss function: Sum of hinge losses for all samples plus L2 regularization\n",
    "        hinge_loss = cp.sum(cp.pos(1 - cp.multiply(y, X @ w + b)))\n",
    "        L2_reg = cp.norm(w, 2)**2\n",
    "        loss = hinge_loss + self.alpha * L2_reg\n",
    "        \n",
    "        # Define and solve the optimization problem\n",
    "        problem = cp.Problem(cp.Minimize(loss))\n",
    "        problem.solve()\n",
    "        \n",
    "        # Store the optimized parameters\n",
    "        self.w = w.value\n",
    "        self.b = b.value\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return the predicted label 1 or -1\"\"\" \n",
    "        y_pred = X @ self.w + self.b\n",
    "        return np.sign(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e67149c-4fe7-4927-bb79-ddaeee9bc784",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a80ed599fd35cb19e922943fbdd63c2",
     "grade": true,
     "grade_id": "cell-ad5e9ad4a32cc501",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BinaryClassificationModel2(alpha=1e-3)\n",
    "model.fit(X4, y4)\n",
    "pred = model.predict(X4)\n",
    "accuracy = accuracy_score(y4, pred)\n",
    "print(accuracy)\n",
    "assert accuracy >= 0.98"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcabe02-54b9-4bc6-9828-672eea928299",
   "metadata": {},
   "source": [
    "# Lasso with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59172a9-e0b1-4746-bece-ac813cf87a48",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73b5a96f62f1d27a1321d4a89969d81c",
     "grade": false,
     "grade_id": "cell-655412d486bd67e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = load_diabetes()\n",
    "X5, y5 = data.data, data.target\n",
    "\n",
    "def mse_loss_vectorized(w, b, X, y):\n",
    "    \"\"\"\n",
    "    MSE loss function WITHOUT FOR LOOPs , NO REGULARIZATION\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - loss \n",
    "    - gradient with respect to weights w\n",
    "    - gradient with respect to bias b\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    dw = np.zeros_like(w)\n",
    "    \n",
    "    loss = np.mean(np.square(X @ w + b - y)) \n",
    "    dw = ((X.T @ (X @ w + b - y)) / X.shape[0])\n",
    "    db =  np.sum(X @ w + b - y) / X.shape[0]\n",
    "    \n",
    "    return loss, dw, np.array(db).reshape(1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d07353-f3b9-473d-a5f1-c98a61aae1f7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38dc493c9ca20537caf2935aa3cf4e1c",
     "grade": false,
     "grade_id": "cell-d98f47532310ef5c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lasso_gradient_mse_loss_vectorized(w, b, X, y, alpha):\n",
    "    \"\"\"\n",
    "    MSE loss function adding the subgradient for w\n",
    "    \"\"\"\n",
    "    loss, dw, db = mse_loss_vectorized(w, b, X, y)\n",
    "    \n",
    "    # Add Lasso regularization penalty to the loss\n",
    "    lasso_penalty = alpha * np.sum(np.abs(w))\n",
    "    loss += lasso_penalty\n",
    "    \n",
    "    # Compute the subgradient for the Lasso penalty and add it to dw\n",
    "    dw += alpha * np.sign(w)\n",
    "    \n",
    "    return loss, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf332e1-d693-4a9f-b895-ab569201b386",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1366a2c529f80b1de518386becbfeb78",
     "grade": false,
     "grade_id": "cell-2aaf96cf6b5bfa0c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LassoGradientDescent():\n",
    "    def __init__(self,  alpha=0.1):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def train(self, X, y, learning_rate=1e-3, num_iters=100, batch_size=200, verbose=False):\n",
    "        N, d = X.shape\n",
    "        \n",
    "        if self.w is None: # Initialization\n",
    "            self.w = 0.001 * np.random.randn(d)\n",
    "            self.b = 0.0\n",
    "\n",
    "        # Run stochastic gradient descent to optimize w\n",
    "        \n",
    "        loss_history = []\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "                                                               \n",
    "            # Sample batch_size elements in X_batch and y_batch\n",
    "            indices = np.random.choice(N, batch_size)\n",
    "            X_batch = X[indices]\n",
    "            y_batch = y[indices]\n",
    "            \n",
    "            # evaluate loss and gradient\n",
    "            loss, dw, db = self.loss(X_batch, y_batch)\n",
    "\n",
    "            # perform parameter update                                                                \n",
    "            # Update the weights w using the gradient and the learning rate.  \n",
    "            self.w -= learning_rate * dw\n",
    "            self.b -= learning_rate * db\n",
    "            \n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            if verbose and it % 10000 == 0:\n",
    "                print(\"iteration %d / %d: loss %f\" % (it, num_iters, loss))\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X @ self.w + self.b\n",
    "\n",
    "    def loss(self, X_batch, y_batch):\n",
    "        return lasso_gradient_mse_loss_vectorized(self.w, self.b, X_batch, y_batch, self.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4269fe85-9f6e-4c58-8838-65c9da22964d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29d39b45b9231a0a6078701b663d790d",
     "grade": true,
     "grade_id": "cell-e774279fbf788990",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LassoGradientDescent(alpha=0.1)\n",
    "model.train(X5, y5, learning_rate=1e-2,verbose=True, num_iters=200_000)\n",
    "pred = model.predict(X5)\n",
    "mse = mean_squared_error(pred, y5)\n",
    "\n",
    "sk_model = Lasso(alpha=0.1, fit_intercept=True)\n",
    "sk_model.fit(X5, y5)\n",
    "sk_pred = sk_model.predict(X5)\n",
    "sk_mse = mean_squared_error(sk_pred, y5)\n",
    "\n",
    "print(\"MSE scikit-learn:\", sk_mse)\n",
    "print(\"MSE Coordinate descent model :\", mse)\n",
    "assert mse - sk_mse < 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836a64f6-d106-4fba-8fa3-036675416f0e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f165a3546dd7c1ac190f380098778a85",
     "grade": true,
     "grade_id": "cell-b3b8578b6e77ce5a",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LassoGradientDescent(alpha=2)\n",
    "model.train(X5, y5, learning_rate=1e-2,verbose=True, num_iters=200_000)\n",
    "pred = model.predict(X5)\n",
    "mse = mean_squared_error(pred, y5)\n",
    "\n",
    "sk_model = Lasso(alpha=2, fit_intercept=True)\n",
    "sk_model.fit(X5, y5)\n",
    "sk_pred = sk_model.predict(X5)\n",
    "sk_mse = mean_squared_error(sk_pred, y5)\n",
    "\n",
    "print(\"MSE scikit-learn:\", sk_mse)\n",
    "print(\"MSE Coordinate descent model :\", mse)\n",
    "assert mse - sk_mse < 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052d4301-e58e-4f36-9d5a-54bfbc4e8f46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
